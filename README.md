# Classification-and-detection-of-electronic-components
Classification and detection of electronic components using CNN

1. Classification
   
    The dataset used in this study consists in 3102 electronic circuit images. The dimensions of each image is 320x320 pixels and represents one of the seven distinct object classes related to electronic components within the circuit: resistors, capacitors, coils, current sources, voltage sources, and alternating current sources.
    For the classifition models, the objects from the images are cropped and saved to another location in order to classify them. A new unbalanced dataset is formed, one objects being represented very little.
    For classification are implemented two types of models: VGG16 and Resnet50. The purpose of this is to see how each of this perform and that the advantages and disadvantages will be clear. The first one implementet is VGG16. Convolutional layers in this model play a crucial role in extracting features from input images through convolution operations. In the first block of convolutions, filters are applied to 60x60 pixel images and shared across the entire image. This 
allows them to capture various portions of the input image, irrespective of its dimensions. As these filters traverse different grid blocks, their receptive fields expand, enabling them to detect more intricate and larger features.
The pooling layers in the VGG16 model play a vital role in decreasing the spatial dimensions of the feature maps while retaining important information. This reduction helps to enhance network efficiency and improve its tolerance towards translations and slight distortions in input data. Each 2D convolution layer reduces the spatial dimensions of the feature maps by half compared to their previous size, further contributing to computational complexity reduction.The VGG16 model includes two densely connected layers at the output, which are subsequently followed by a Softmax layer. In our implemented model, we have removed the convolutional training basis - specifically, the last three layers of VGG16 - resulting in zero learnable parameters for this particular aspect. This technique is called transfer learning. The last three layers are replaced with a custom model. The flattening layer is an important component in convolutional neural network architecture as it transforms the output of the convolutional layers into a vector that can be passed to the fully connected layers. A L2 regularization is applied to prevent overfitting by penalizing large weights. To further combat overfitting, a dropout layer is implemented which randomly removes some neurons from the network during training, introducing noise and enhancing generalization capabilities. The ReLU activation function introduces nonlinearity into the network by setting negative values to zero while preserving positive values. This allows for increased flexibility and improved learning capacity.The finaly layer is responsible for making the prediction. The Resnet50 model consists in five rezidual blocks. which allows extremely deep neural networks to be trained without the problem of gradients diminishing as they propagate back through multiple layers.
    The residual blocks are composed of 3 convolution layers. In the first block is a 1x1 layer with 128 filters, a 3x3 layer with the same number of filters and a 1x1 layer with 256 filters. This block is repeated three times. Residual blocks three, four, and five have an increased number of filters, being multiplied by four in the second block, six in the third, and three in the fourth. Image is halfed after each residual block. Just like the VGG16 model, the transfer learning technique its used. The final layers are replaced with a the same model that replaced the final layers of VGG16, to help preventing overfitting.

2. Detection

    In order to enhance the performance of this model, a new background class is implemented in addition to the existing seven electronics-specific classes. This inclusion enables the model to effectively differentiate between areas without objects and areas with potential objects. The model implemented in this study is a faster R-CNN that has been pretrained with both the Resnet50 and VGG16 backbones. Firstly, the VGG16 is used as backbone, which is a less compex 
arhitecure. After that, the Resnet50 pretrained model is used as backbone aswell, which is the more conservative approach. The arhitecture of both approaches is the same except the backbone frame. Resnet50 includes a FPN arhitecture aswell. To select this particular model, we have defined a "get_model" function which also retrieves the number of input features needed for the box predictor head. After obtaining this information, we replace the current box predictor in the region of interest with a new one created using the "FastRCNNPredictor" class. The value assigned to "in_features" corresponds to the input size required for our new box predictor implementation. The input image is processed by the pretrained backbone network, which includes five residual blocks. After each block, the image is downscaled to capture important information. These blocks are capable of capturing both low-level features such as textures and edges, as well as high-level features like object shapes. RPN utilizes this extracted information. Additionally, the backbone network incorporates a feature pyramid network to enhance the model's object detection capabilities across different scales. FPN leverages features from Resnet50 at various levels of abstraction and spatial resolutions through convolution layers while ensuring consistency in channel numbers across feature maps. By fusing higher resolution layer features with lower 
resolution ones, FPN improves representation of features at all scales. The second stage includes the deployment of the detection network. With the ROI pooling layer, regions of interest are generated, which are potential locations of previously detected objects. A multi-scale region-of-interest alignment operation is applied to these to adjust the regions to a constant size. Thus, regions of interest are compatible and coherent regardless of their initial size. 
Next, MLP layers are applied to extract features of regions of interest. The last step is the prediction of the class scores and the displacement of the bounding boxes. Classor scores refer to the probability that an object belongs to a particular class, while box offsets indicate how the predicted bounding box differs from the correct one
